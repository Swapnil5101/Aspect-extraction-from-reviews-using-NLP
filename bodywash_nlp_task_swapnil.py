# -*- coding: utf-8 -*-
"""Bodywash_NLP_task_Swapnil.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16SpeRB2SC1_TyE-Nb8BVo9nZ41dGWBWn

## Import necessary libraries
"""

import numpy as np
import pandas as pd
import string
import re
from tqdm import tqdm
import matplotlib.pyplot as plt

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import torch
from torch.utils.data import Dataset, DataLoader, TensorDataset

import nltk
from nltk.corpus import stopwords
from transformers import RobertaTokenizer
from transformers import RobertaForSequenceClassification
from transformers import AdamW

import warnings
warnings.filterwarnings('ignore')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

"""## Load and look train dataset"""

df = pd.read_excel('/content/bodywash-train .xlsx')
df.head()

df.tail(10)

df['Level 1 Factors'].unique()

test_df = pd.read_excel('/content/bodywash-test .xlsx')
test_df.head()

"""## Preprocessing"""

# RENAME columns for ease of use
df.rename(columns={'Core Item': 'text', 'Level 1 Factors': 'tags'}, inplace=True)
df.iloc[10:20]

# # Keep a copy of df that might be needed later!
# df_copy = df.copy()

df.iloc[12]['text']

df.iloc[28]['text']

df.shape

df.isnull().sum()

# df = df_copy   ## if needed

# Group by 'text' (as some texts are repeated) and aggregate 'tags' into lists
df = df.groupby('text', as_index=False).agg({'tags': lambda x: list(set(x))})  # Use set to avoid duplicates in tags
df.reset_index(drop=True, inplace=True)

df.shape

def handle_dollar_amounts(text):
    # Handle $X format
    text = re.sub(r'\$(\d+\.?\d*)', r'\1 dollars', text)
    # Handle X$ format
    text = re.sub(r'(\d+\.?\d*)\$', r'\1 dollars', text)
    return text

# Preprocessing function. MAY/MAY NOT REMOVE STOPWORDS
def preprocess_text(text):
    orig_text = text           # for ref.

    # Handle hashtags: Remove leading '#' and split by camel casing, underscores, and numbers
    text = re.sub(r'#', '', text)
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)  # Split camel casing
    text = re.sub(r'[_\W]+', ' ', text)              # Remove special chars except '@'
    text = re.sub(r'(\d+)', r' \1 ', text)           # Separate numbers

    # Clean up extra spaces and lowercase
    text =  ' '.join(text.split()).lower()


    # Tokenize
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    # tokens = [word for word in tokens if word not in stopwords.words('english')]

    # Reconstruct text from tokens
    return ' '.join(tokens)

# Apply preprocessing to the 'text' column
df['text'] = df['text'].apply(preprocess_text)

df.head(10)

"""### Encoding 'tags' column"""

mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['tags'])

y.shape

# df_copy['tags'].unique()

"""## Feature Representation

### TF-IDF
"""

tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features
X_tfidf = tfidf_vectorizer.fit_transform(df['text'])

"""### Word2Vec"""

from gensim.models import Word2Vec

# Tokenize the cleaned texts for Word2Vec training
tokenized_texts = [text.split() for text in df['text']]

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)

# Function to create feature vectors for each document
def document_vector(doc):
    # Remove out-of-vocab words
    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]
    return np.mean(word2vec_model.wv[doc], axis=0) if doc else np.zeros(100)

X_w2v = np.array([document_vector(text.split()) for text in df['text']])

"""## Train, predict and evaluate (on validation data)

###  TF-IDF + LinearSVC model
"""

X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)   # Split data

# Train Linear SVC model
svc_model = MultiOutputClassifier(LinearSVC())
svc_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred_svc = svc_model.predict(X_val)
print(classification_report(y_val, y_pred_svc, target_names=mlb.classes_))

"""### Word2Vec + LinearSVC model"""

X_train_w2v, X_val_w2v, y_train_w2v, y_val_w2v = train_test_split(X_w2v, y, test_size=0.2, random_state=42)   # Split data

# Train Linear SVC model with Word2Vec
svc_model_w2v = MultiOutputClassifier(LinearSVC())
svc_model_w2v.fit(X_train_w2v, y_train_w2v)

# Predictions and evaluation
y_pred_svc_w2v = svc_model_w2v.predict(X_val_w2v)
print(classification_report(y_val_w2v, y_pred_svc_w2v, target_names=mlb.classes_))

"""### Random Forest Classifier with TF-IDF"""

rf_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100))
rf_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred_rf = rf_model.predict(X_val)
print(classification_report(y_val, y_pred_rf, target_names=mlb.classes_))

"""### Random Forest Classifier with Word2Vec"""

rf_model_w2v = MultiOutputClassifier(RandomForestClassifier(n_estimators=100))
rf_model_w2v.fit(X_train_w2v, y_train_w2v)

y_pred_rf_w2v = rf_model_w2v.predict(X_val_w2v)
print(classification_report(y_val_w2v, y_pred_rf_w2v, target_names=mlb.classes_))

"""## Model Training with RoBERTa

There is one thing evident from above classification reports that "Class imbalance" is an issue here that should be dealt. Let's do it using 'balanced' class weights (giving importance to minority classes).
"""

y.shape

## MEMOERY ISSUES ON MY SYSTEM WITH THIS APPROACH

# # Compute class weights for multi-label classification
# class_counts = y.sum(axis=0)  # Sum occurrences of each class
# total_samples = y.shape[0]   # Total number of samples
# class_weights = total_samples / (len(class_counts) * class_counts)  # Inverse frequency

# # Convert to PyTorch tensor
# class_weights = torch.tensor(class_weights, dtype=torch.float)

# class_weights.shape

"""### Tokenize with RoBERTa and load model"""

# Load the RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization with RoBERTa
inputs = tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)

# Load the RoBERTa model for multi-label classification
roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(mlb.classes_))

# Freeze initial layers (e.g., first 6 layers)
for param in roberta_model.roberta.encoder.layer[:6]:
    for p in param.parameters():
        p.requires_grad = False

# print(inputs)

"""### Dataloader preparation"""

X_train_R, X_val_R, y_train_R, y_val_R = train_test_split(df['text'], y, test_size=0.2, random_state=42)

# Tokenize both train and validation data
train_inputs_R = tokenizer(X_train_R.tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)
val_inputs_R = tokenizer(X_val_R.tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)

# TensorDatasets
train_dataset = TensorDataset(train_inputs_R['input_ids'], train_inputs_R['attention_mask'], torch.FloatTensor(y_train_R))
val_dataset = TensorDataset(val_inputs_R['input_ids'], val_inputs_R['attention_mask'], torch.FloatTensor(y_val_R))

#  DataLoaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

"""### RoBERTa training loop"""

roberta_model = roberta_model.to(device)

optimizer = AdamW(roberta_model.parameters(), lr=1e-4)

num_epochs = 3

roberta_model.train()
for epoch in range(num_epochs):
    for batch in tqdm(train_loader):
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        # Forward pass
        outputs = roberta_model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        # Backward pass and optimization step
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}')
    print('-'*50)
    print()

# Save roberta_model locally
save_path = "./roberta_model"  # Specify save path
tokenizer.save_pretrained(save_path)
roberta_model.save_pretrained(save_path)  # Save the best roberta_roberta_model

# # Define loss function with class weights
# loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights.to(device))

# # When dealing with class imbalance by computing class weights and using BCEWithLogitsLoss as loss func
# # MEMORY ISSUES ON MY SYSTEM, SO GOING WITHOUT THIS APPROACH, BUT MENTIONED FOR REF.

# roberta_model.train()
# best_val_loss = float('inf')
# num_epochs = 3

# for epoch in range(num_epochs):  # Set a higher number of epochs initially
#     total_loss = 0

#     for batch_idx, batch in enumerate(train_loader):
#         optimizer.zero_grad()
#         input_ids, attention_mask, labels = batch
#         labels = labels.float()  # Ensure labels are float
#         print(labels.shape)
#         input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

#         # Forward pass
#         outputs = roberta_model(input_ids, attention_mask=attention_mask)
#         print(outputs.logits.shape)
#         loss = loss_fn(outputs.logits, labels)

#         # Backward pass and optimization step
#         loss.backward()
#         optimizer.step()

#         total_loss += loss.item()

#     avg_train_loss = total_loss / len(train_loader)

#     # Validation step
#     roberta_model.eval()
#     val_loss = 0

#     with torch.no_grad():
#         for batch in val_loader:
#             input_ids, attention_mask, labels = batch

#             outputs = roberta_model(input_ids, attention_mask=attention_mask)
#             loss = loss_fn(outputs.logits, labels)
#             val_loss += loss.item()

#     avg_val_loss = val_loss / len(val_loader)

#     print(f'Epoch: {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')
#     print('-'*50)
#     print()

#     # Check for improvement in validation loss
#     if val_loss < best_val_loss:
#         best_val_loss = val_loss

#     # Save roberta_model locally
#     save_path = "./roberta_model"  # Specify save path
#     roberta_model.save_pretrained(save_path)  # Save the best roberta_roberta_model

"""### Predict and Evaluate RoBERTa on validation set"""

# # Load the model and tokenizer
# roberta_model = RobertaForSequenceClassification.from_pretrained(save_path)
# tokenizer = RobertaTokenizer.from_pretrained(save_path)

X_train_R, X_val_R, y_train_R, y_val_R = train_test_split(df['text'], y, test_size=0.2, random_state=42)

val_inputs = tokenizer(X_val_R.tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)

roberta_model.eval()  # evaluation mode

with torch.no_grad():
    roberta_val_outputs = roberta_model(val_inputs['input_ids'].to(device), attention_mask=val_inputs['attention_mask'].to(device))
    roberta_preds = torch.sigmoid(roberta_val_outputs.logits) > 0.5  # Thresholding for multi-label classification

# Convert predictions back to labels
predicted_labels = mlb.inverse_transform(roberta_preds.cpu().numpy())

# Evaluate
print("classification report of RoBERTa model:\n", classification_report(y_val_R, roberta_preds.cpu().numpy(), target_names=mlb.classes_))

test_texts = test_df['Core Item'].tolist()

# Load the RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Preprocess and tokenize test data
test_inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors="pt", max_length=512)

# Set model to evaluation mode
roberta_model.eval()

# Make predictions
with torch.no_grad():
    outputs = roberta_model(test_inputs['input_ids'].to(device), attention_mask=test_inputs['attention_mask'].to(device))
    predictions = torch.sigmoid(outputs.logits) > 0.5  # Apply threshold for multi-label classification

# Convert predictions to numpy array
predicted_labels = predictions.cpu().numpy()

# Inverse transform to get actual labels
predicted_tags = mlb.inverse_transform(predicted_labels)

# results DataFrame
results_df = pd.DataFrame({
    'Core Item': test_texts,
    'Predicted Tags': [', '.join(tags) for tags in predicted_tags]
})

results_df